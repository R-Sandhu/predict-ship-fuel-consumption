{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import reader\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    \"\"\"\n",
    "    Load data from a csv file \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filenanme: str\n",
    "      Complete path of input dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "      List of lists where each element represents a single record\n",
    "    \"\"\"\n",
    "    dataset = list()\n",
    "    with open(fileName, \"r\") as f:\n",
    "        csv_reader = reader(f)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Raw Data as Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '../data/raw/ship_data.csv'\n",
    "dataset = load_csv(fileName)\n",
    "print(\"Loaded data with {0} rows and {1} columns\".format(len(dataset), len(dataset[0])))\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "colNames = dataset[0]\n",
    "df = pd.DataFrame(dataset[1:], columns=colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to convert unix timestamp to date-time object and re-index data frame, rename dataFrame columns to more readable names for easy referencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_index(data, col_ts, ts_units):\n",
    "    \"\"\"\n",
    "    Convert unix ts to date-time and re-index data-frame \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Pandas DataFrame\n",
    "      Input pandas data frame object\n",
    "    col_ts: str\n",
    "      Column name with timestamp data\n",
    "    ts_units: str\n",
    "      Units of the timestamp. This could be 's' as seconds, refer to documentation for more options.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "      Pandas DataFrame object with dateTime index\n",
    "    \"\"\"\n",
    "    data[col_ts] = pd.to_datetime(data[col_ts], unit=ts_units)\n",
    "    data.set_index(col_ts, inplace=True)\n",
    "    return data\n",
    "\n",
    "def rename_cols(data, new_col_names):\n",
    "    \"\"\"\n",
    "    Rename columns \n",
    "    \"\"\"\n",
    "    data.columns = new_col_names\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = time_index(df, 'Time', 's')\n",
    "new_col_names = ['fuelConsumption', 'HFO', 'MGO', 'draftForward', 'draftAft', 'draftMid1', 'draftMid2',\n",
    "              'shaftSpeed', 'shaftTorque', 'shaftPower', 'speedGround', 'speedWater', 'heading', 'rudderAngle',\n",
    "              'AWS', 'AWD', 'TWS', 'TWD', 'temp', 'currentDirection', 'currentSpeed', 'waterDepth', 'waveHeight',\n",
    "              'wavePeriod', 'waveDirection']\n",
    "\n",
    "df = rename_cols(df, new_col_names)\n",
    "df.iloc[:6, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 0)\n",
    "pd.set_option('precision', 4)\n",
    "description = dataset.describe()\n",
    "description.iloc[:, :11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 3)\n",
    "cols_to_drop = ['HFO', 'MGO', 'currentDirection', 'currentSpeed', 'waterDepth', 'waveHeight', 'wavePeriod', \n",
    "                'waveDirection']\n",
    "correlations = dataset.drop(cols_to_drop, axis=1).corr(method='pearson')\n",
    "correlations.iloc[:10, :11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,17,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(dataset.drop(cols_to_drop, axis=1).columns, rotation=90)\n",
    "ax.set_yticklabels(dataset.drop(cols_to_drop, axis=1).columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness of univariate distributions\n",
    "dataset.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='density', subplots=True, layout=(5,5), sharex=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='box', subplots=True, layout=(5,5), sharex=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values count plot\n",
    "nan_all = dataset.isna().sum()\n",
    "missing = nan_all[nan_all != 0].sort_values()\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(missing.index, missing.values, color='gray')\n",
    "ax.set_xlabel(\"Fig1: Missing values count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero values counts plot\n",
    "zeroCounts_all = (dataset == 0).sum().sort_values()\n",
    "zeroCounts = zeroCounts_all[zeroCounts_all > 500]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(zeroCounts.index, zeroCounts.values, color='gray')\n",
    "ax.set_xlabel(\"Fig2: Zero values count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meanDraft'] = df[['draftAft', 'draftForward', 'draftMid1', 'draftMid2']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with high number of values zero - MGO, waveDirection, wavePeriod, waveHeight, waterDepth,\n",
    "# currentDirection, currentSpeed, speedGround. Shaft power is output rather than input, shaft torque is not known\n",
    "# before, so drop these too. AWS & AWD are dropped as their calculation involves speed of ship.\n",
    "cols_to_drop = ['MGO', 'draftForward', 'draftAft', 'draftMid1', 'draftMid2', 'shaftTorque', 'shaftPower',\n",
    "                'speedGround', 'AWS', 'AWD', 'currentDirection', 'currentSpeed', 'waterDepth',\n",
    "                'waveHeight', 'wavePeriod', 'waveDirection']\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['fuelConsumption', 'speedWater', 'shaftSpeed', 'temp']].plot(subplots=True, figsize=(15, 10))\n",
    "plt.xlabel(\"Fig3: Year and month (dateTime)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['meanDraft', 'heading', 'rudderAngle', 'TWS']].plot(subplots=True, figsize=(15, 10))\n",
    "plt.xlabel(\"Fig4: Year and month (dateTime)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics - before data cleaning\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Absolute temp values (observed < -273), which are likely because of sensor error. Drop negative heading\n",
    "# values and fuel consumption values < -0.5\n",
    "conditionEval = (df['heading'] < 0) | (df['temp'] < -273) | (df['fuelConsumption'] < -0.5)\n",
    "df.drop(df[conditionEval].index, inplace=True)\n",
    "# Replace the Fuel consumption negative values between -0.5 and 0 with zero. These values are likely to be within\n",
    "# standard error of measurement. Since shaft speed is zero for these values, it is inferred that the ship is\n",
    "# docked at port.\n",
    "df.loc[df['fuelConsumption'] < 0, 'fuelConsumption'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics - after data cleaning\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "sns.set_palette(\"RdBu_r\")\n",
    "g = sns.pairplot(df[df['fuelConsumption'] > 0], vars=['fuelConsumption', 'speedWater', 'shaftSpeed', 'temp'],\n",
    "                 plot_kws={'alpha': 0.4})\n",
    "g.fig.suptitle(\"Fig5: Univariate & bivariate distributions\",  y=1.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "sns.heatmap(df.drop(columns='HFO').corr(), annot=True, cbar=False, linewidths=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(color='grey');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "dataArray = df.values\n",
    "X = dataArray[:, 1:10]\n",
    "y = dataArray[:, 0]\n",
    "validation_size = 0.2\n",
    "seed = 1\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "seed = 1\n",
    "scoring = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LinearRegression())) \n",
    "models.append(('LASSO', Lasso())) \n",
    "models.append(('EN', ElasticNet())) \n",
    "models.append(('KNN', KNeighborsRegressor())) \n",
    "models.append(('CART', DecisionTreeRegressor())) \n",
    "models.append(('SVR', SVR()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure() \n",
    "fig.suptitle('Algorithm Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results, showmeans=True) \n",
    "ax.set_xticklabels(names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())]))) \n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison') \n",
    "ax = fig.add_subplot(111) \n",
    "plt.boxplot(results) \n",
    "ax.set_xticklabels(names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = []\n",
    "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\n",
    "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=10))]))) \n",
    "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor(n_estimators=10))])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Ensemble Algorithm Comparison') \n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore Number of trees and evaluate performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # define number of trees to consider\n",
    "    n_trees = [10, 50, 100, 300, 500, 1000, 1500, 2000]\n",
    "    for n in n_trees:\n",
    "        models[str(n)] = ExtraTreesRegressor(n_estimators=n)\n",
    "    return models\n",
    "\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models.items():\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, rescaledX, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore number of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Explore number of features to consider from 1 to 8\n",
    "    for i in range(1, rescaledX.shape[1]):\n",
    "        models[str(i)] = ExtraTreesRegressor(max_features=i)\n",
    "    return models\n",
    "\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models.items():\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, rescaledX, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore minimum samples per split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Explore minimum samples per split from 2 to 8\n",
    "    for i in range(2, 9):\n",
    "        models[str(i)] = ExtraTreesRegressor(min_samples_split=i)\n",
    "    return models\n",
    "\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models.items():\n",
    "  kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "  cv_results = cross_val_score(model, rescaledX, y_train, cv=kfold, scoring=scoring, n_jobs=14)\n",
    "  results.append(cv_results)\n",
    "  names.append(name)\n",
    "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "  print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model\n",
    "model = ExtraTreesRegressor(random_state=seed, \n",
    "                            n_estimators=100, \n",
    "                            max_features=5, \n",
    "                            min_samples_split=5, \n",
    "                            n_jobs=14)\n",
    "model.fit(rescaledX, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the validation dataset\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(mean_squared_error(y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x = predictions, y = y_validation);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
